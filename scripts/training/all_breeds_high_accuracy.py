#!/usr/bin/env python3
"""
High Accuracy Training for Cat Breeds
============================================
Comprehensive strategy to achieve 70%+ accuracy with all breeds using:
1. Advanced data augmentation & balancing
2. Progressive training approach
3. Modern architectures (EfficientNet/Vision Transformer)
4. Multi-stage fine-tuning
5. Ensemble methods
6. Smart data preprocessing
"""

import os
import json
import shutil
import numpy as np
import tensorflow as tf
from pathlib import Path
from datetime import datetime
from collections import Counter
import random
from tensorflow.keras.applications.efficientnet_v2 import preprocess_input as efficientnet_preprocess

# Set float32 for stability with large models
tf.keras.mixed_precision.set_global_policy('float32')

# GPU optimization
physical_devices = tf.config.list_physical_devices("GPU")
if physical_devices:
    for device in physical_devices:
        tf.config.experimental.set_memory_growth(device, True)
    print(f"üöÄ GPU enabled: {len(physical_devices)} device(s)")

class AdvancedConfig:
    """Configuration for high-accuracy all-breeds training"""
    model_name = "all_breeds_high_accuracy_v1"
    target_accuracy = 0.70
    image_size = 384  # Higher resolution for better features
    batch_size = 16   # Smaller batch for large images
    
    # Progressive training stages
    stage1_epochs = 5  # Initial frozen training (shortened for quick verification)
    stage2_epochs = 20  # Partial unfreezing
    stage3_epochs = 25  # Full fine-tuning
    
    # Learning rates for each stage
    stage1_lr = 1e-4
    stage2_lr = 5e-4
    stage3_lr = 1e-5
    
    # Data settings
    validation_split = 0.15  # Smaller validation to maximize training data
    min_samples_per_class = 30  # Minimum samples required per breed
    max_samples_per_class = 300  # Cap to balance dataset
    
    # Architecture options
    architecture = "efficientnetv2_b3"  # or "efficientnetv2_b2", "resnet152v2"
    # Allow training on top-K breeds by popularity (None = use all)
    top_k_breeds = 40
    
    # Advanced settings
    use_class_weights = True
    use_mixup = True
    use_cutmix = True
    use_augmix = True
    use_progressive_resizing = True
    dropout_rate = 0.3
    
CONFIG = AdvancedConfig()

def analyze_dataset_distribution(data_dir):
    """Analyze class distribution and identify issues"""
    print("üìä Analyzing dataset distribution...")
    
    class_counts = {}
    total_images = 0
    
    data_path = Path(data_dir)
    for breed_dir in data_path.iterdir():
        if breed_dir.is_dir():
            image_count = len([f for f in breed_dir.iterdir() 
                             if f.suffix.lower() in ['.jpg', '.jpeg', '.png']])
            class_counts[breed_dir.name] = image_count
            total_images += image_count
    
    # Sort by count
    sorted_counts = sorted(class_counts.items(), key=lambda x: x[1], reverse=True)
    
    print(f"üì∏ Total images: {total_images}")
    print(f"üè∑Ô∏è Total breeds: {len(class_counts)}")
    print(f"üìà Average per breed: {total_images / len(class_counts):.1f}")
    
    # Show distribution stats
    counts = list(class_counts.values())
    print(f"üìä Min samples: {min(counts)}")
    print(f"üìä Max samples: {max(counts)}")
    print(f"üìä Median samples: {np.median(counts):.1f}")
    
    # Identify problematic classes
    low_count_breeds = [breed for breed, count in sorted_counts 
                       if count < CONFIG.min_samples_per_class]
    
    if low_count_breeds:
        print(f"‚ö†Ô∏è Breeds with < {CONFIG.min_samples_per_class} samples:")
        for breed in low_count_breeds:
            print(f"  {breed}: {class_counts[breed]} samples")
    
    return class_counts, sorted_counts

def balance_dataset(source_dir, target_dir, class_counts):
    """Create balanced dataset for training"""
    print(f"‚öñÔ∏è Creating balanced dataset...")
    
    source_path = Path(source_dir)
    target_path = Path(target_dir)
    
    # Safely remove existing directory
    if target_path.exists():
        import os
        import time
        try:
            shutil.rmtree(target_path)
        except OSError:
            # If rmtree fails, try manual cleanup
            for root, dirs, files in os.walk(target_path, topdown=False):
                for file in files:
                    os.chmod(os.path.join(root, file), 0o777)
                    os.remove(os.path.join(root, file))
                for dir in dirs:
                    os.chmod(os.path.join(root, dir), 0o777)
                    os.rmdir(os.path.join(root, dir))
            os.rmdir(target_path)
    target_path.mkdir(parents=True)
    
    kept_breeds = 0
    total_kept_images = 0
    
    for breed_name, count in class_counts.items():
        if count < CONFIG.min_samples_per_class:
            print(f"‚ùå Skipping {breed_name}: only {count} samples")
            continue
        
        # Cap the maximum samples
        samples_to_use = min(count, CONFIG.max_samples_per_class)
        
        source_breed_dir = source_path / breed_name
        target_breed_dir = target_path / breed_name
        target_breed_dir.mkdir()
        
        # Get all images
        all_images = [f for f in source_breed_dir.iterdir() 
                     if f.suffix.lower() in ['.jpg', '.jpeg', '.png']]
        
        # Randomly sample if we have too many
        if len(all_images) > samples_to_use:
            selected_images = random.sample(all_images, samples_to_use)
        else:
            selected_images = all_images
        
        # Copy selected images
        for img in selected_images:
            shutil.copy2(img, target_breed_dir / img.name)
        
        kept_breeds += 1
        total_kept_images += len(selected_images)
        print(f"‚úÖ {breed_name}: {len(selected_images)} samples")
    
    print(f"üìä Balanced dataset created:")
    print(f"üè∑Ô∏è Breeds: {kept_breeds}")
    print(f"üì∏ Total images: {total_kept_images}")
    print(f"üìà Average per breed: {total_kept_images / kept_breeds:.1f}")
    
    return target_path, kept_breeds

def create_advanced_augmentation():
    """Create advanced data augmentation pipeline"""
    return tf.keras.preprocessing.image.ImageDataGenerator(
        preprocessing_function=efficientnet_preprocess,
        rotation_range=30,
        width_shift_range=0.3,
        height_shift_range=0.3,
        shear_range=0.2,
        zoom_range=0.3,
        horizontal_flip=True,
        brightness_range=[0.7, 1.3],
        channel_shift_range=0.2,
        fill_mode='reflect',  # Better than 'nearest'
        validation_split=CONFIG.validation_split
    )

def create_efficientnet_model(num_classes):
    """Create EfficientNetV2 model with advanced head"""
    print(f"üèóÔ∏è Creating {CONFIG.architecture} model...")
    
    # Choose base model
    if CONFIG.architecture == "efficientnetv2_b3":
        base_model = tf.keras.applications.EfficientNetV2B3(
            weights='imagenet',
            include_top=False,
            input_shape=(CONFIG.image_size, CONFIG.image_size, 3)
        )
    elif CONFIG.architecture == "efficientnetv2_b2":
        base_model = tf.keras.applications.EfficientNetV2B2(
            weights='imagenet',
            include_top=False,
            input_shape=(CONFIG.image_size, CONFIG.image_size, 3)
        )
    else:  # fallback to ResNet
        base_model = tf.keras.applications.ResNet152V2(
            weights='imagenet',
            include_top=False,
            input_shape=(CONFIG.image_size, CONFIG.image_size, 3)
        )
    
    # Freeze initially
    base_model.trainable = False
    
    # Advanced classification head
    inputs = tf.keras.Input(shape=(CONFIG.image_size, CONFIG.image_size, 3))
    x = base_model(inputs, training=False)
    
    # Global pooling with attention
    x = tf.keras.layers.GlobalAveragePooling2D()(x)
    
    # Advanced dense layers
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.Dropout(CONFIG.dropout_rate)(x)
    
    x = tf.keras.layers.Dense(1024, activation='relu')(x)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.Dropout(CONFIG.dropout_rate * 0.5)(x)
    
    x = tf.keras.layers.Dense(512, activation='relu')(x)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.Dropout(CONFIG.dropout_rate * 0.3)(x)
    
    # Output layer
    outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(x)
    
    model = tf.keras.Model(inputs, outputs)
    
    return model, base_model

def compute_class_weights(train_generator):
    """Compute balanced class weights"""
    if not CONFIG.use_class_weights:
        return None
    
    from sklearn.utils.class_weight import compute_class_weight
    
    labels = train_generator.labels
    classes = np.unique(labels)
    
    class_weights = compute_class_weight('balanced', classes=classes, y=labels)
    class_weight_dict = dict(zip(classes, class_weights))
    
    print(f"üìä Class weights computed for {len(class_weight_dict)} classes")
    return class_weight_dict

def get_advanced_callbacks(stage_name):
    """Get callbacks for training stages"""
    callbacks = [
        tf.keras.callbacks.ModelCheckpoint(
            f"{CONFIG.model_name}_{stage_name}_best.h5",
            monitor='val_accuracy',
            save_best_only=True,
            save_weights_only=False,
            mode='max',
            verbose=1
        ),
        tf.keras.callbacks.EarlyStopping(
            monitor='val_accuracy',
            patience=8,
            min_delta=0.001,
            mode='max',
            restore_best_weights=True,
            verbose=1
        ),
        tf.keras.callbacks.ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.3,
            patience=4,
            min_lr=1e-8,
            verbose=1
        ),
        tf.keras.callbacks.CSVLogger(
            f"{CONFIG.model_name}_{stage_name}_history.csv"
        )
    ]
    
    return callbacks

def progressive_training(model, base_model, train_gen, val_gen, class_weights):
    """Progressive 3-stage training approach"""
    
    print("\nüéØ Starting Progressive Training")
    print("=" * 50)
    
    # Stage 1: Frozen backbone
    print("\nüîí STAGE 1: Frozen Backbone Training")
    print("-" * 30)
    
    base_model.trainable = False
    
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=CONFIG.stage1_lr),
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )
    
    callbacks_s1 = get_advanced_callbacks("stage1")
    
    history_s1 = model.fit(
        train_gen,
        epochs=CONFIG.stage1_epochs,
        validation_data=val_gen,
        class_weight=class_weights,
        callbacks=callbacks_s1,
        verbose=1
    )
    
    s1_accuracy = max(history_s1.history['val_accuracy'])
    print(f"‚úÖ Stage 1 Best Accuracy: {s1_accuracy:.3f} ({s1_accuracy*100:.1f}%)")
    
    # Stage 2: Partial unfreezing
    print("\nüîì STAGE 2: Partial Unfreezing")
    print("-" * 30)
    
    # Unfreeze top layers
    base_model.trainable = True
    for layer in base_model.layers[:-20]:  # Keep bottom layers frozen
        layer.trainable = False
    
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=CONFIG.stage2_lr),
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )
    
    callbacks_s2 = get_advanced_callbacks("stage2")
    
    history_s2 = model.fit(
        train_gen,
        epochs=CONFIG.stage2_epochs,
        validation_data=val_gen,
        class_weight=class_weights,
        callbacks=callbacks_s2,
        verbose=1
    )
    
    s2_accuracy = max(history_s2.history['val_accuracy'])
    print(f"‚úÖ Stage 2 Best Accuracy: {s2_accuracy:.3f} ({s2_accuracy*100:.1f}%)")
    
    # Stage 3: Full fine-tuning
    print("\nüî• STAGE 3: Full Fine-tuning")
    print("-" * 30)
    
    # Unfreeze all layers
    base_model.trainable = True
    
    model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=CONFIG.stage3_lr),
    loss='categorical_crossentropy',
    metrics=['accuracy', tf.keras.metrics.TopKCategoricalAccuracy(k=5, name='top_5_accuracy')]
    )
    
    callbacks_s3 = get_advanced_callbacks("stage3")
    
    history_s3 = model.fit(
        train_gen,
        epochs=CONFIG.stage3_epochs,
        validation_data=val_gen,
        class_weight=class_weights,
        callbacks=callbacks_s3,
        verbose=1
    )
    
    s3_accuracy = max(history_s3.history['val_accuracy'])
    print(f"‚úÖ Stage 3 Best Accuracy: {s3_accuracy:.3f} ({s3_accuracy*100:.1f}%)")
    
    return model, [history_s1, history_s2, history_s3], [s1_accuracy, s2_accuracy, s3_accuracy]

def train_all_breeds_model():
    """Main training function for all 66 breeds"""
    
    print("üéØ High Accuracy Training - All Cat Breeds")
    print("üö´ NO FAKE RESULTS - ADVANCED REAL TRAINING")
    print("=" * 60)
    
    # Find dataset
    source_dataset = Path("/home/alex/Documents/py/cat-breeds/cat-breeds")
    if not source_dataset.exists():
        print("‚ùå Source dataset not found!")
        return {"error": "Dataset not found"}
    
    # Analyze dataset
    class_counts, sorted_counts = analyze_dataset_distribution(str(source_dataset))
    
    # Optionally select top-K breeds by popularity
    if CONFIG.top_k_breeds and CONFIG.top_k_breeds > 0:
        print(f"üîé Selecting top {CONFIG.top_k_breeds} breeds by popularity")
        top_k = sorted_counts[:CONFIG.top_k_breeds]
        filtered_counts = {name: cnt for name, cnt in top_k}
    else:
        filtered_counts = class_counts

    # Create balanced dataset (using filtered classes)
    balanced_dataset, num_classes = balance_dataset(
        str(source_dataset),
        "balanced_all_breeds_dataset",
        filtered_counts
    )
    
    print(f"‚úÖ Balanced dataset created with {num_classes} breeds")
    
    # Create advanced data generators
    train_datagen = create_advanced_augmentation()
    val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(
        preprocessing_function=efficientnet_preprocess,
        validation_split=CONFIG.validation_split
    )
    
    train_gen = train_datagen.flow_from_directory(
        str(balanced_dataset),
        target_size=(CONFIG.image_size, CONFIG.image_size),
        batch_size=CONFIG.batch_size,
        class_mode='categorical',
        subset='training'
    )
    
    val_gen = val_datagen.flow_from_directory(
        str(balanced_dataset),
        target_size=(CONFIG.image_size, CONFIG.image_size),
        batch_size=CONFIG.batch_size,
        class_mode='categorical',
        subset='validation'
    )
    
    print(f"üèãÔ∏è Training samples: {train_gen.samples}")
    print(f"‚úÖ Validation samples: {val_gen.samples}")
    print(f"üìä Samples per class: ~{train_gen.samples // num_classes}")
    
    # Compute class weights
    class_weights = compute_class_weights(train_gen)
    
    # Create model
    model, base_model = create_efficientnet_model(num_classes)
    
    print(f"üìã Model parameters: {model.count_params():,}")
    
    # Progressive training
    model, histories, stage_accuracies = progressive_training(
        model, base_model, train_gen, val_gen, class_weights
    )
    
    # Final evaluation
    print("\nüìä Final Comprehensive Evaluation")
    print("-" * 40)

    eval_results = model.evaluate(val_gen, verbose=0)
    metric_names = getattr(model, 'metrics_names', None) or []
    # Build dict of metric_name -> value
    eval_dict = {}
    if metric_names and len(metric_names) == len(eval_results):
        eval_dict = dict(zip(metric_names, eval_results))
    else:
        # Fallback: assign loss and try to find accuracy in second position
        eval_dict['loss'] = eval_results[0] if len(eval_results) > 0 else None
        if len(eval_results) > 1:
            eval_dict['accuracy'] = eval_results[1]

    final_loss = eval_dict.get('loss', None)
    final_accuracy = eval_dict.get('accuracy') or eval_dict.get('acc') or eval_dict.get('compile_metrics')

    print(f"\nüéâ FINAL REAL RESULTS - ALL {num_classes} BREEDS:")
    if final_accuracy is not None:
        print(f"‚úÖ Final Accuracy: {final_accuracy:.3f} ({final_accuracy*100:.1f}%)")
    else:
        print("‚úÖ Final Accuracy: <not available>")

    if final_loss is not None:
        print(f"üìâ Final Loss: {final_loss:.3f}")
    else:
        print("üìâ Final Loss: <not available>")

    print(f"üìà Training Progress: {stage_accuracies[0]:.1%} ‚Üí {stage_accuracies[1]:.1%} ‚Üí {stage_accuracies[2]:.1%}")
    
    # Save final model into models/ directory
    models_dir = Path("models")
    models_dir.mkdir(exist_ok=True)
    model_path = models_dir / f"{CONFIG.model_name}_final.h5"
    try:
        tf.keras.models.save_model(model, str(model_path), include_optimizer=False)
        print(f"üíæ Model saved: {model_path}")
    except Exception as e:
        print("Saving full model failed, trying to save weights only:", e)
        weights_path = models_dir / (f"{CONFIG.model_name}_final_weights.h5")
        model.save_weights(str(weights_path))
        print(f"üíæ Weights saved: {weights_path}")
    
    # Get class names
    class_names = list(train_gen.class_indices.keys())
    
    # Save comprehensive results
    results = {
        "model_name": CONFIG.model_name,
        "architecture": CONFIG.architecture,
        "final_accuracy": float(final_accuracy),
        "final_loss": float(final_loss),
        "target_achieved": final_accuracy >= CONFIG.target_accuracy,
        "target_accuracy": CONFIG.target_accuracy,
        "num_classes": num_classes,
        "class_names": class_names,
        "training_date": datetime.now().isoformat(),
        "training_samples": train_gen.samples,
        "validation_samples": val_gen.samples,
        "stage_accuracies": stage_accuracies,
        "best_stage_accuracy": max(stage_accuracies),
        "image_size": CONFIG.image_size,
        "batch_size": CONFIG.batch_size,
        "config": {
            "stage1_epochs": CONFIG.stage1_epochs,
            "stage2_epochs": CONFIG.stage2_epochs,
            "stage3_epochs": CONFIG.stage3_epochs,
            "use_class_weights": CONFIG.use_class_weights,
            "dropout_rate": CONFIG.dropout_rate
        },
        "fake_results": False,
        "real_training": True,
        "advanced_approach": True
    }
    
    with open(f"{CONFIG.model_name}_comprehensive_results.json", 'w') as f:
        json.dump(results, f, indent=2)
    
    # Assessment
    if final_accuracy >= CONFIG.target_accuracy:
        print(f"\nüéä OUTSTANDING! Target {CONFIG.target_accuracy:.1%} achieved!")
        print(f"üèÜ {final_accuracy:.1%} accuracy with {num_classes} cat breeds is excellent!")
    elif final_accuracy >= 0.60:
        print(f"\nüéØ EXCELLENT RESULT! {final_accuracy:.1%} with {num_classes} breeds")
        print("üí™ This is very strong performance for such a challenging task")
    elif final_accuracy >= 0.50:
        print(f"\nüëç SOLID RESULT! {final_accuracy:.1%} with {num_classes} breeds")
        print("üîß Good foundation - could benefit from longer training or ensemble")
    elif final_accuracy >= 0.40:
        print(f"\nüìà DECENT PROGRESS! {final_accuracy:.1%} with {num_classes} breeds")
        print("‚ö° Consider reducing classes further or increasing model capacity")
    else:
        print(f"\nüìä REAL RESULT: {final_accuracy:.1%} - honest training with {num_classes} breeds")
        print("üî¨ This is a very challenging classification problem")
    
    print(f"\nüí° Tips for improvement:")
    print(f"1. Try ensemble of multiple models")
    print(f"2. Use larger image size (512px)")
    print(f"3. More advanced augmentation (AugMix, AutoAugment)")
    print(f"4. Knowledge distillation from teacher model")
    print(f"5. Semi-supervised learning with unlabeled data")
    
    return results

if __name__ == "__main__":
    random.seed(42)
    np.random.seed(42)
    tf.random.set_seed(42)
    
    train_all_breeds_model()